%%%%% GPT-1
@article{gpt1,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  year={2018},
}

%%%%% GPT-2
@article{gpt2,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI Blog},
  year={2019}
}

%%%%% GPT-3
@article{gpt3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

%%%%% GPT-3.5
@article{gpt35,
  title={Introducing ChatGPT},
  author={OpenAI},
  journal={OpenAI Blog Nov 30 2022},
  url={https://openai.com/index/chatgpt/},
  year={2022}
}

%%%%% GPT-4
@article{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

%%%%% GPT-4o
@article{gpt4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}

%%%%% GPT-4o-Mini
@article{gpt4omini,
  title={GPT-4o mini: advancing cost-efficient intelligence},
  author={OpenAI},
  journal={OpenAI Blog Jul 18 2024},
  url={https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/},
  year={2024}
}

%%%%% GPT-4.1
@article{gpt41,
  title={Introducing GPT-4.1 in the API},
  author={OpenAI},
  journal={OpenAI Blog Apr 14 2025},
  url={https://openai.com/index/gpt-4-1/},
  year={2025}
}

%%%%% GPT-4.5
@article{gpt45,
  title={Introducing GPT-4.5},
  author={OpenAI},
  journal={OpenAI Blog Feb 27 2025},
  url={https://openai.com/index/introducing-gpt-4-5/},
  year={2025}
}

%%%%% GPT-5 / GPT-5-Mini
@article{gpt5-gpt5mini,
  title={Introducing GPT-5},
  author={OpenAI},
  journal={OpenAI Blog Aug 7 2025},
  url={https://openai.com/index/introducing-gpt-5/},
  year={2025}
}

%%%%% GPT-5.1
@article{gpt51,
  title={GPT-5.1: A smarter, more conversational ChatGPT},
  author={OpenAI},
  journal={OpenAI Blog Nov 12 2025},
  url={https://openai.com/index/gpt-5-1/},
  year={2025}
}

%%%%% GPT-5.2
@article{gpt52,
  title={Introducing GPT-5.2},
  author={OpenAI},
  journal={OpenAI Blog Dec 11 2025},
  url={https://openai.com/index/introducing-gpt-5-2/},
  year={2025}
}

%%%%% o1
@article{o1,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

%%%%% o3 / o4-Mini
@article{o3-o4mini,
  title={Introducing OpenAI o3 and o4-mini},
  author={OpenAI},
  journal={OpenAI Blog Apr 16 2025},
  url={https://openai.com/index/introducing-o3-and-o4-mini/},
  year={2025}
}

%%%%% Genimi 1.0
@article{gemini10,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

%%%%% Genimi 1.5
@article{gemini15,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

%%%%% Genimi 2.0
@article{gemini20,
  title={Introducing Gemini 2.0: our new AI model for the agentic era},
  author={Pichai, Sundar and Hassabis, Demis and Kavukcuoglu, Koray},
  journal={Google Blog Dec 11 2024},
  url={https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/},
  year={2024}
}

%%%%% Genimi 2.5
@article{gemini25,
  title={Gemini 2.5: Our most intelligent AI model},
  author={Kavukcuoglu, Koray},
  journal={Google Blog Mar 25 2025},
  url={https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/},
  year={2025}
}

%%%%% Genimi 3.0 Pro
@article{gemini30pro,
  title={A new era of intelligence with Gemini 3},
  author={Pichai, Sundar and Hassabis, Demis and Kavukcuoglu, Koray},
  journal={Google Blog Nov 18 2025},
  url={https://blog.google/products/gemini/gemini-3/},
  year={2025}
}

%%%%% Genimi 3.0 Flash
@article{gemini30flash,
  title={Gemini 3 Flash: frontier intelligence built for speed},
  author={Doshi, Tulsee},
  journal={Google Blog Dec 17 2025},
  url={https://blog.google/products/gemini/gemini-3-flash/},
  year={2025}
}

%%%%% Gemma
@article{gemma,
  title={Gemma: Open models based on gemini research and technology},
  author={Team, Gemma and Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Pathak, Shreya and Sifre, Laurent and Rivi{\`e}re, Morgane and Kale, Mihir Sanjay and Love, Juliette and others},
  journal={arXiv preprint arXiv:2403.08295},
  year={2024}
}

%%%%% Gemma 2
@article{gemma2,
  title={Gemma 2: Improving open language models at a practical size},
  author={Team, Gemma and Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}

%%%%% Gemma 3
@article{gemma3,
  title={Gemma 3 technical report},
  author={Team, Gemma and Kamath, Aishwarya and Ferret, Johan and Pathak, Shreya and Vieillard, Nino and Merhej, Ramona and Perrin, Sarah and Matejovicova, Tatiana and Ram{\'e}, Alexandre and Rivi{\`e}re, Morgane and others},
  journal={arXiv preprint arXiv:2503.19786},
  year={2025}
}

%%%%% Claude 3.5
@article{claude35,
  title={Claude 3.5 Sonnet},
  author={Anthropic},
  journal={Anthropic Blog Jun 20 2024},
  url={https://www.anthropic.com/news/claude-3-5-sonnet},
  year={2024}
}

%%%%% Claude 3.7
@article{claude37,
  title={Claude 3.7 Sonnet and Claude Code},
  author={Anthropic},
  journal={Anthropic Blog Feb 24 2025},
  url={https://www.anthropic.com/news/claude-3-7-sonnet},
  year={2025}
}

%%%%% Claude 4
@article{claude4,
  title={Introducing Claude 4},
  author={Anthropic},
  journal={Anthropic Blog Mar 22 2025},
  url={https://www.anthropic.com/news/claude-4},
  year={2025}
}

%%%%% Claude 4.5
@article{claude45,
  title={Introducing Claude Sonnet 4.5},
  author={Anthropic},
  journal={Anthropic Blog Sep 29 2025},
  url={https://www.anthropic.com/news/claude-sonnet-4-5},
  year={2025}
}

%%%%% DeepSeek-V3
@article{deepseekv3,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

%%%%% DeepSeek-V3.1
@article{deepseekv31,
  title={DeepSeek-V3.1 Release},
  author={Team, DeepSeek},
  journal={Anthropic Blog Aug 21 2025},
  url={https://api-docs.deepseek.com/news/news250821},
  year={2025}
}

%%%%% DeepSeek-V3.2
@article{deepseekv32,
  title={DeepSeek-V3. 2: Pushing the Frontier of Open Large Language Models},
  author={Liu, Aixin and Mei, Aoxue and Lin, Bangcai and Xue, Bing and Wang, Bingxuan and Xu, Bingzheng and Wu, Bochao and Zhang, Bowei and Lin, Chaofan and Dong, Chen and others},
  journal={arXiv preprint arXiv:2512.02556},
  year={2025}
}

%%%%% DeepSeek-R1
@article{deepseekr1,
  title={DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Zhang, Ruoyu and Ma, Shirong and Bi, Xiao and others},
  journal={Nature},
  volume={645},
  number={8081},
  pages={633--638},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

%%%%% DeepSeek-VL
@article{deepseekvl,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}

%%%%% DeepSeek-VL2
@article{deepseekvl2,
  title={Deepseek-vl2: Mixture-of-experts vision-language models for advanced multimodal understanding},
  author={Wu, Zhiyu and Chen, Xiaokang and Pan, Zizheng and Liu, Xingchao and Liu, Wen and Dai, Damai and Gao, Huazuo and Ma, Yiyang and Wu, Chengyue and Wang, Bingxuan and others},
  journal={arXiv preprint arXiv:2412.10302},
  year={2024}
}

%%%%% Qwen
@article{qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

%%%%% Qwen-2
@article{qwen2,
  title={Qwen2 technical report},
  author={Team, Qwen},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

%%%%% Qwen-2.5
@article{qwen25,
  title={Qwen2.5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

%%%%% Qwen-3
@article{qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

%%%%% QwQ
@article{qwq,
  title={QwQ: Reflect Deeply on the Boundaries of the Unknown},
  author={Team, Qwen},
  journal={Qwen Blogs Nov 28 2024},
  url={https://qwenlm.github.io/blog/qwq-32b-preview/},
  year={2024}
}

%%%%% Qwen-VL
@article{qwenvl,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

%%%%% Qwen2-VL
@article{qwen2vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

%%%%% Qwen-2.5-VL
@article{qwen25vl,
  title={Qwen2.5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

%%%%% Qwen-3-VL
@article{qwen3vl,
  title={Qwen3-VL: Sharper Vision, Deeper Thought, Broader Action},
  author={Team, Qwen},
  journal={Qwen Blogs Sep 22 2025},
  url={https://qwen.ai/blog?id=qwen3-vl},
  year={2025}
}

%%%%% Qwen-VL-Max / Qwen-VL-Plus
@article{qwenvlmax-qwenvlplus,
  title={Introducing Qwen-VL},
  author={Team, Qwen},
  journal={Qwen Blogs Jan 2024},
  url={https://qwenlm.github.io/blog/qwen-vl/},
  year={2024}
}

%%%%% Grok 3
@article{grok3,
  title={Grok 3 Beta â€” The Age of Reasoning Agents},
  author={xAI},
  journal={xAI Blogs Feb 29 2025},
  url={https://x.ai/news/grok-3},
  year={2025}
}

%%%%% Grok 4
@article{grok4,
  title={Grok 4},
  author={xAI},
  journal={xAI Blogs Jul 09 2025},
  url={https://x.ai/news/grok-4},
  year={2025}
}

%%%%% Grok 4.1
@article{grok41,
  title={Grok 4.1},
  author={xAI},
  journal={xAI Blogs Nov 17 2025},
  url={https://x.ai/news/grok-4-1},
  year={2025}
}

%%%%% LLaMA
@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

%%%%% LLaMA-2
@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

%%%%% LLaMA-3
@article{llama3,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

%%%%% LLaMA-3.1
@article{llama31,
  title={Introducing Llama 3.1: Our most capable models to date},
  author={Meta},
  journal={Meta Blog Jul 23 2024},
  url={https://ai.meta.com/blog/meta-llama-3-1/},
  year={2024}
}

%%%%% LLaMA-3.2
@article{llama32,
  title={Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
  author={Meta},
  journal={Meta Blog Sep 25 2024},
  url={https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/},
  year={2024}
}

%%%%% LLaMA-4
@article{llama4,
  title={The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation},
  author={Meta},
  journal={Meta Blog Apr 5 2025},
  url={https://ai.meta.com/blog/llama-4-multimodal-intelligence/},
  year={2025}
}

%%%%% LLaVA-1
@article{llava10,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

%%%%% LLaVA-1.5
@inproceedings{llava15,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={26296--26306},
  year={2024}
}

%%%%% LLaVA-1.6
@misc{llava16,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    month={January},
    year={2024}
}

%%%%% SEED 1.5
@article{seed15vl,
  title={Seed1.5-vl technical report},
  author={Guo, Dong and Wu, Faming and Zhu, Feida and Leng, Fuxing and Shi, Guang and Chen, Haobin and Fan, Haoqi and Wang, Jian and Jiang, Jianyu and Wang, Jiawei and others},
  journal={arXiv preprint arXiv:2505.07062},
  year={2025}
}

%%%%% SEED 1.6
@article{seed16,
  title={Introduction to Techniques Used in Seed1.6},
  author={ByteDance},
  journal={ByteDance Seed Blog Jun 25 2025},
  url={https://seed.bytedance.com/en/seed1_6},
  year={2025}
}

%%%%% SEED 1.8
@article{seed18,
  title={Seed1.8 Model Card: Towards Generalized Real-World Agency},
  author={ByteDance},
  journal={ByteDance Seed Blog Dec 18 2025},
  url={https://seed.bytedance.com/en/seed1_8},
  year={2025}
}

%%%%% Kimi-VL
@article{kimivl,
  title={Kimi-vl technical report},
  author={Team, Kimi and Du, Angang and Yin, Bohong and Xing, Bowei and Qu, Bowen and Wang, Bowen and Chen, Cheng and Zhang, Chenlin and Du, Chenzhuang and Wei, Chu and others},
  journal={arXiv preprint arXiv:2504.07491},
  year={2025}
}

%%%%% Kimi-k1.5
@article{kimik15,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}

%%%%% Kimi-k2
@article{kimik2,
  title={Kimi k2: Open agentic intelligence},
  author={Team, Kimi and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and others},
  journal={arXiv preprint arXiv:2507.20534},
  year={2025}
}

%%%%% Moonshot-V1-Vision-Preview
@article{moonshotv1vision,
  title={Multimodal Image Understanding Model Moonshot-V1-Vision-Preview},
  author={MoonshotAI},
  journal={Moonshot AI Blogs Jan 2025},
  url={https://platform.moonshot.cn/docs/guide/use-kimi-vision-model},
  year={2025}
}

%%%%% GLM-4
@article{glm4,
  title={Chatglm: A family of large language models from glm-130b to glm-4 all tools},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

%%%%% GLM-4.5
@article{glm45,
  title={Glm-4.5: Agentic, reasoning, and coding (arc) foundation models},
  author={Zeng, Aohan and Lv, Xin and Zheng, Qinkai and Hou, Zhenyu and Chen, Bin and Xie, Chengxing and Wang, Cunxiang and Yin, Da and Zeng, Hao and Zhang, Jiajie and others},
  journal={arXiv preprint arXiv:2508.06471},
  year={2025}
}

%%%%% GLM-4.6
@article{glm46,
  title={GLM-4.6: Advanced Agentic, Reasoning and Coding Capabilities},
  author={Z.ai},
  journal={Z.ai Blogs Sep 30 2025},
  url={https://z.ai/blog/glm-4.6},
  year={2025}
}

%%%%% GLM-4.7
@article{glm47,
  title={GLM-4.7: Advancing the Coding Capability},
  author={Z.ai},
  journal={Z.ai Blogs Dec 22 2025},
  url={https://z.ai/blog/glm-4.7},
  year={2025}
}

%%%%% GLM-4.5V / GLM-4.1V-Thinking
@article{glm45v-glm41vthinking,
  title={GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning},
  author={Wenyi Hong and Wenmeng Yu and Xiaotao Gu and Guo Wang and Guobing Gan and Haomiao Tang and Jiale Cheng and Ji Qi and Junhui Ji and Lihang Pan and others},
  journal={arXiv preprint arXiv:2507.01006},
  year={2025},
}

%%%%% GLM-4.6V
@article{glm46v,
  title={GLM-4.6V: Open Source Multimodal Models with Native Tool Use},
  author={Z.ai},
  journal={Z.ai Blogs Dec 08 2025},
  url={https://z.ai/blog/glm-4.6v},
  year={2025}
}

%%%%% Mistral
@article{mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

%%%%% Mixtral
@article{mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

%%%%% WizardLM
@inproceedings{wizardlm,
  title={WizardLM: Empowering large pre-trained language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

%%%%% WizardLM
@article{wizardlm2,
  title={WizardLM 2},
  author={{WizardLM Team}},
  journal={Blog Apr 15 2024},
  url={https://wizardlm.github.io/WizardLM2/},
  year={2024}
}

%%%%% Phi-4
@article{phi4,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}

%%%%% Phi-4-Reasoning
@article{phi4reasoning,
  title={Phi-4-reasoning technical report},
  author={Abdin, Marah and Agarwal, Sahaj and Awadallah, Ahmed and Balachandran, Vidhisha and Behl, Harkirat and Chen, Lingjiao and de Rosa, Gustavo and Gunasekar, Suriya and Javaheripi, Mojan and Joshi, Neel and others},
  journal={arXiv preprint arXiv:2504.21318},
  year={2025}
}

%%%%% Phi-4-Mini / Phi-4-Multimodal
@article{phi4mini-phi4multimodal,
  title={Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras},
  author={Abouelenin, Abdelrahman and Ashfaq, Atabak and Atkinson, Adam and Awadalla, Hany and Bach, Nguyen and Bao, Jianmin and Benhaim, Alon and Cai, Martin and Chaudhary, Vishrav and Chen, Congcong and others},
  journal={arXiv preprint arXiv:2503.01743},
  year={2025}
}

%%%%% Vicuna
@article{vicuna,
  title={Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
  journal={LMSYS ORG},
  url={https://lmsys.org/blog/2023-03-30-vicuna/},
  year={2023}
}

%%%%% Olmo 1
@inproceedings{olmo1,
  title={OLMo: Accelerating the science of language models},
  author={Groeneveld, Dirk and Beltagy, Iz and Walsh, Evan and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and others},
  booktitle={Proceedings of the 62nd annual meeting of the association for computational linguistics (volume 1: Long papers)},
  pages={15789--15809},
  year={2024}
}

%%%%% Olmo 2
@inproceedings{olmo2,
  title={2 OLMo 2 Furious},
  author={Walsh, Evan Pete and Soldaini, Luca and Groeneveld, Dirk and Lo, Kyle and Arora, Shane and Bhagia, Akshita and Gu, Yuling and Huang, Shengyi and Jordan, Matt and Lambert, Nathan and others},
  booktitle={Second Conference on Language Modeling},
  year={2025}
}

%%%%% Olmo 3
@article{olmo3,
  title={Olmo 3: Charting a path through the model flow to lead open-source AI},
  author={Team, Olmo},
  journal={AI2 Blog Nov 30 2025},
  url={https://allenai.org/blog/olmo3},
  year={2025}
}

%%%%% CLIP
@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

%%%%% Midjourney
@misc{midjourney,
  title={Midjourney},
  author={{Midjourney Inc.}},
  year={2022},
  url={https://www.midjourney.com/home}
}

%%%%% SD1
@inproceedings{sd1,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}

%%%%% SD2
@article{sd2,
  title={Stable Diffusion 2.0 Release},
  author={stability.ai},
  journal={stability.ai Blog Nov 23 2022},
  url={https://stability.ai/news/stable-diffusion-v2-release},
  year={2022}
}

%%%%% SD3
@inproceedings{sd3,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first international conference on machine learning},
  year={2024}
}

%%%%% SD3.5
@article{sd35,
  title={Introducing Stable Diffusion 3.5},
  author={stability.ai},
  journal={stability.ai Blog Oct 22 2024},
  url={https://stability.ai/news/introducing-stable-diffusion-3-5},
  year={2022}
}

%%%%% SDXL
@inproceedings{sdxl,
  title={SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

%%%%% Dall-E 1
@article{dalle1,
  title={Dall-E: Creating Images from Text},
  author={{OpenAI}},
  journal={OpenAI Blog Jan 5 2021},
  url={https://openai.com/index/dall-e/},
  year={2021}
}

%%%%% Dall-E 2
@article{dalle2,
  title={Dall-E 2},
  author={{OpenAI}},
  journal={OpenAI Blog Apr 2022},
  url={https://openai.com/index/dall-e-2/},
  year={2022}
}

%%%%% Dall-E 3
@article{dalle3,
  title={Dall-E 3},
  author={{OpenAI}},
  journal={OpenAI Blog Oct 2023},
  url={https://openai.com/index/dall-e-3/},
  year={2023}
}

%%%%% Flux 1
@article{flux1,
  title={Announcing Black Forest Labs},
  author={{Black Forest Labs}},
  journal={BFL Blog Aug 1 2024},
  url={https://bfl.ai/blog/24-08-01-bfl},
  year={2024}
}

%%%%% Flux 1.1
@article{flux11,
  title={Announcing FLUX1.1 [pro] and the BFL API},
  author={{Black Forest Labs}},
  journal={BFL Blog Oct 2 2024},
  url={https://bfl.ai/blog/24-10-02-flux},
  year={2024}
}

%%%%% Transformers
@article{transformers,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
